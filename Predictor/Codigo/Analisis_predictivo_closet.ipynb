{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "source": [
        "## Data loading\n",
        "\n",
        "### Subtask:\n",
        "Load the data from \"wc-product-export-20-3-2025-1742505370284.csv\" into a pandas DataFrame.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "qTcseKnHQulK"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Load the data from the CSV file into a pandas DataFrame.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "WwwlsrWBQu07"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('')\n",
        "display(df.head())"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "NNI8Ubl7QvEn",
        "outputId": "b23dd726-e101-4465-cd61-295d38fda489"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Semestre 7/wc-product-export-20-3-2025-1742505370284.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-3863335302.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Semestre 7/wc-product-export-20-3-2025-1742505370284.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Semestre 7/wc-product-export-20-3-2025-1742505370284.csv'"
          ]
        }
      ]
    },
    {
      "source": [
        "## Data exploration\n",
        "\n",
        "### Subtask:\n",
        "Explore the loaded data to understand its structure, identify key features, and understand the distribution of variables.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "R4mdU7OcQyWy"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Explore the basic structure and descriptive statistics of the dataframe to understand the data.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "0QluErykQzJ0"
      }
    },
    {
      "source": [
        "# Data Shape\n",
        "print(f\"The DataFrame has {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
        "\n",
        "# Data Types\n",
        "print(\"\\nData Types of Each Column:\\n\", df.dtypes)\n",
        "\n",
        "# Descriptive Statistics for Numerical Features\n",
        "numerical_features = df.select_dtypes(include=['number'])\n",
        "print(\"\\nDescriptive Statistics for Numerical Features:\\n\", numerical_features.describe())\n",
        "\n",
        "# Missing Values\n",
        "print(\"\\nMissing Values in Each Column:\\n\", df.isnull().sum())\n",
        "\n",
        "# Unique Values for Categorical Features\n",
        "categorical_features = df.select_dtypes(include=['object'])\n",
        "for column in categorical_features.columns:\n",
        "  print(f\"\\nUnique values for {column}: {df[column].nunique()}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "jc4hjKGsQzZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Analyze the distribution of key variables such as 'Precio normal' and 'Precio rebajado' using histograms.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "mWB_JUmqQ06f"
      }
    },
    {
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Distribution of 'Precio normal'\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(df['Precio normal'], bins=20)\n",
        "plt.xlabel('Precio normal')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Precio normal')\n",
        "plt.show()\n",
        "\n",
        "# Distribution of 'Precio rebajado'\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(df['Precio rebajado'], bins=20)\n",
        "plt.xlabel('Precio rebajado')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Precio rebajado')\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "jOeJrshmQ1KT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Calculate and visualize the correlation matrix between numerical features to identify potential relationships.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "s1E-7YgaQ27d"
      }
    },
    {
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Correlation Matrix\n",
        "correlation_matrix = numerical_features.corr()\n",
        "\n",
        "# Visualize the Correlation Matrix\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix of Numerical Features')\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "2MB72IZFQ3LO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Analyze the trends and patterns over time if there is a date/time column in the dataframe.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "ItQyUQCuQ5gT"
      }
    },
    {
      "source": [
        "# Check if there is a date/time column\n",
        "datetime_columns = [col for col in df.columns if 'Día' in col]\n",
        "\n",
        "if datetime_columns:\n",
        "    # Convert the date/time column to datetime objects\n",
        "    for column in datetime_columns:\n",
        "        df[column] = pd.to_datetime(df[column], errors='coerce')\n",
        "\n",
        "    # Analyze trends over time (e.g., sales over time)\n",
        "    # Assuming 'Día en que empieza el precio rebajado' is a relevant date column\n",
        "    if 'Día en que empieza el precio rebajado' in df.columns:\n",
        "        sales_over_time = df.groupby('Día en que empieza el precio rebajado')['Precio normal'].sum()\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(sales_over_time.index, sales_over_time.values)\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Total Sales')\n",
        "        plt.title('Sales Over Time')\n",
        "        plt.show()\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "10xRzq0rQ5wL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Data cleaning\n",
        "\n",
        "### Subtask:\n",
        "Clean the data by handling missing values, removing outliers, and dealing with inconsistencies in the data.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "Sej9BPbKQ9Bw"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Start by handling missing values by calculating the percentage of missing values in each column and then decide on strategies for dealing with them based on the percentage.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "pWjAkG8rQ93B"
      }
    },
    {
      "source": [
        "# Calculate the percentage of missing values in each column\n",
        "missing_percentage = df.isnull().sum() * 100 / len(df)\n",
        "\n",
        "# Print the percentage of missing values for each column\n",
        "print(\"Percentage of missing values in each column:\\n\", missing_percentage)\n",
        "\n",
        "# Remove rows with a small percentage of missing values (e.g., less than 5%)\n",
        "df = df.dropna(thresh=len(df.columns) - int(0.05 * len(df.columns)))\n",
        "\n",
        "# For columns with a larger percentage of missing values, consider imputation strategies\n",
        "# Example: Impute missing values in 'Precio normal' with the median\n",
        "if 'Precio normal' in df.columns and df['Precio normal'].isnull().any():\n",
        "  df['Precio normal'].fillna(df['Precio normal'].median(), inplace=True)\n",
        "\n",
        "# Example: Impute missing values in 'Descripción' with the most frequent value\n",
        "if 'Descripción' in df.columns and df['Descripción'].isnull().any():\n",
        "  df['Descripción'].fillna(df['Descripción'].mode()[0], inplace=True)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "93RzNWGgQ-Gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Continue with outlier detection and handling for numerical features.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "pr4MGCVfQ_p4"
      }
    },
    {
      "source": [
        "# Identify numerical features\n",
        "numerical_features = df.select_dtypes(include=['number'])\n",
        "\n",
        "# Loop through numerical features and handle outliers\n",
        "for column in numerical_features.columns:\n",
        "  # Check if the column has any data\n",
        "  if not df[column].empty:\n",
        "    # Calculate the IQR\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # Define the lower and upper bounds for outliers\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Identify outliers\n",
        "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
        "\n",
        "    # Handle outliers (e.g., remove rows containing outliers)\n",
        "    if not outliers.empty:\n",
        "      df = df.drop(outliers.index)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "hqVtzQdLQ_5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Now, we need to deal with inconsistencies in categorical variables. We can start by examining the unique values for each categorical feature and look for spelling errors, inconsistent formatting, or duplicate categories.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5kA9mdURBWq"
      }
    },
    {
      "source": [
        "# Identify categorical features\n",
        "categorical_features = df.select_dtypes(include=['object'])\n",
        "\n",
        "# Loop through categorical features and handle inconsistencies\n",
        "for column in categorical_features.columns:\n",
        "  # Check if the column has any data\n",
        "  if not df[column].empty:\n",
        "    # Convert values to lowercase and remove leading/trailing whitespace\n",
        "    df[column] = df[column].str.lower().str.strip()\n",
        "\n",
        "    # Replace specific inconsistencies (e.g., misspellings)\n",
        "    # Example: Replace \"bolsas\" with \"bolsa\"\n",
        "    if 'Nombre' in column:\n",
        "      df[column] = df[column].replace('bolsas', 'bolsa')\n",
        "\n",
        "    # Check for duplicate categories and merge them\n",
        "    unique_values = df[column].unique()\n",
        "    for value in unique_values:\n",
        "        # Check if a similar value exists\n",
        "        similar_values = [val for val in unique_values if val != value and value in val or val in value]\n",
        "        if similar_values:\n",
        "            # Replace similar values with the most frequent value\n",
        "            df[column] = df[column].replace(similar_values, value)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "3k--vlbVRBmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Data Wrangling\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "NKSqhkvnRaj6"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the data again\n",
        "df = pd.read_csv('/content/drive/MyDrive/Semestre 7/wc-product-export-20-3-2025-1742505370284.csv')\n",
        "\n",
        "# Impute missing values in 'Precio normal' with the median\n",
        "if 'Precio normal' in df.columns and df['Precio normal'].isnull().any():\n",
        "  df['Precio normal'] = df['Precio normal'].fillna(df['Precio normal'].median())\n",
        "\n",
        "# Impute missing values in 'Precio rebajado' with the median\n",
        "if 'Precio rebajado' in df.columns and df['Precio rebajado'].isnull().any():\n",
        "  df['Precio rebajado'] = df['Precio rebajado'].fillna(df['Precio rebajado'].median())\n",
        "\n",
        "# Impute missing values in 'Inventario' with the median\n",
        "if 'Inventario' in df.columns and df['Inventario'].isnull().any():\n",
        "  df['Inventario'] = df['Inventario'].fillna(df['Inventario'].median())\n",
        "\n",
        "\n",
        "# Feature engineering: Create a new column representing the \"product category\" based on the product names.\n",
        "if 'Nombre' in df.columns:\n",
        "  # Try different methods to extract product categories\n",
        "  df['product_category'] = df['Nombre'].str.lower().str.extract(r'(camisetas|pantalones|vestidos|zapatos|bolsos)')\n",
        "  # If the previous method doesn't work, try another one\n",
        "  df.loc[df['product_category'].isnull(), 'product_category'] = df.loc[df['product_category'].isnull(), 'Nombre'].str.lower().str.split().str[0]\n",
        "  # If the previous method doesn't work, try another one\n",
        "  df.loc[df['product_category'].isnull(), 'product_category'] = 'Otros'\n",
        "\n",
        "# Feature engineering: Create a new feature indicating whether the product is on sale or not.\n",
        "if 'Precio rebajado' in df.columns and 'Precio normal' in df.columns:\n",
        "  df['on_sale'] = df['Precio rebajado'] < df['Precio normal']\n",
        "\n",
        "# Feature engineering: Transform the date columns into features like \"month\", \"day of week\", \"year\" and \"quarter\" to capture potential seasonality effects.\n",
        "if 'Día en que empieza el precio rebajado' in df.columns:\n",
        "  df['Día en que empieza el precio rebajado'] = pd.to_datetime(df['Día en que empieza el precio rebajado'], errors='coerce')\n",
        "  df['month'] = df['Día en que empieza el precio rebajado'].dt.month\n",
        "  df['day_of_week'] = df['Día en que empieza el precio rebajado'].dt.dayofweek\n",
        "  df['year'] = df['Día en que empieza el precio rebajado'].dt.year\n",
        "  df['quarter'] = df['Día en que empieza el precio rebajado'].dt.quarter\n",
        "\n",
        "# Data aggregation: Group data by product category and calculate the total sales\n",
        "if 'product_category' in df.columns and 'Precio normal' in df.columns:\n",
        "  sales_by_category = df.groupby('product_category')['Precio normal'].sum()\n",
        "  print(sales_by_category)\n",
        "\n",
        "# Feature selection: Select the features that are most relevant for the predictive model.\n",
        "# For simplicity, select a few relevant features based on domain knowledge.\n",
        "relevant_features = ['Precio normal', 'Precio rebajado', 'Inventario', 'product_category', 'on_sale', 'month', 'day_of_week', 'year', 'quarter']\n",
        "df_selected = df[relevant_features]\n",
        "\n",
        "# Display the first few rows of the prepared data\n",
        "display(df_selected.head())"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "8u4_2rF7Razu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Data splitting\n",
        "\n",
        "### Subtask:\n",
        "Split the prepared data into training, validation, and testing sets.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "LcV6ZnRORedH"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Split the `df_selected` dataframe into training, validation, and testing sets using `train_test_split`.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "tKhWdsAMRes_"
      }
    },
    {
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "df_train, df_test = train_test_split(df_selected, test_size=0.3, random_state=42)\n",
        "\n",
        "# Further split the training set into training and validation sets\n",
        "df_train, df_val = train_test_split(df_train, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "xl3lHU8JRe8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Model optimization\n",
        "\n",
        "### Subtask:\n",
        "Optimize the trained linear regression model to improve its performance.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "iOM88iebRrLK"
      }
    },
    {
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Separate the target variable and predictor variables\n",
        "X_train = df_train.drop(['Precio normal', 'month', 'day_of_week', 'year', 'quarter'], axis=1)\n",
        "y_train = df_train['Precio normal']\n",
        "X_val = df_val.drop(['Precio normal', 'month', 'day_of_week', 'year', 'quarter'], axis=1)\n",
        "y_val = df_val['Precio normal']\n",
        "\n",
        "# Create a column transformer to one-hot encode categorical features and impute missing values\n",
        "categorical_features = ['product_category']\n",
        "numerical_features = ['Precio rebajado', 'Inventario', 'on_sale']\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "        ]), categorical_features),\n",
        "        ('num', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='mean'))\n",
        "        ]), numerical_features)\n",
        "    ])\n",
        "\n",
        "# Create a pipeline for the model\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "# Define the hyperparameter grid (remove 'normalize' parameter)\n",
        "param_grid = {\n",
        "    'regressor__fit_intercept': [True, False]\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the validation set using the best model\n",
        "y_pred = best_model.predict(X_val)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "mae = mean_absolute_error(y_val, y_pred)\n",
        "rmse = mean_squared_error(y_val, y_pred) ** 0.5\n",
        "\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
        "print(f\"Best Hyperparameters: {grid_search.best_params_}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "zidIdjVURur6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "**Reasoning**:\n",
        "The model performance can be further improved by considering feature engineering. We can create new features that might be more predictive, such as interaction terms or polynomial features.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "laI7LpSURwY_"
      }
    },
    {
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Separate the target variable and predictor variables\n",
        "X_train = df_train.drop(['Precio normal', 'month', 'day_of_week', 'year', 'quarter'], axis=1)\n",
        "y_train = df_train['Precio normal']\n",
        "X_val = df_val.drop(['Precio normal', 'month', 'day_of_week', 'year', 'quarter'], axis=1)\n",
        "y_val = df_val['Precio normal']\n",
        "\n",
        "# Create a column transformer to one-hot encode categorical features and impute missing values\n",
        "categorical_features = ['product_category']\n",
        "numerical_features = ['Precio rebajado', 'Inventario', 'on_sale']\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "        ]), categorical_features),\n",
        "        ('num', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='mean'))\n",
        "        ]), numerical_features)\n",
        "    ])\n",
        "\n",
        "# Create a pipeline for the model with polynomial features\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('poly', PolynomialFeatures(degree=2)),  # Add polynomial features\n",
        "    ('regressor', LinearRegression(fit_intercept=False))  # Use the best hyperparameter from previous step\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_pred = pipeline.predict(X_val)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "mae = mean_absolute_error(y_val, y_pred)\n",
        "rmse = mean_squared_error(y_val, y_pred) ** 0.5\n",
        "\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "dfcIr-_LRwou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "**Reasoning**:\n",
        "The model's performance has improved after adding polynomial features. Now, we should analyze the feature importance to understand the impact of different features on the prediction and identify potentially irrelevant features.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "EVHRQA00RyKL"
      }
    },
    {
      "source": [
        "# Get the coefficients of the linear regression model\n",
        "coefficients = pipeline.named_steps['regressor'].coef_\n",
        "\n",
        "# Get the feature names after preprocessing\n",
        "feature_names = pipeline.named_steps['poly'].get_feature_names_out(\n",
        "    pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
        ")\n",
        "\n",
        "# Create a DataFrame to store the coefficients and feature names\n",
        "feature_importance = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
        "\n",
        "# Sort the DataFrame by the absolute value of the coefficients\n",
        "feature_importance['Absolute Coefficient'] = feature_importance['Coefficient'].abs()\n",
        "feature_importance = feature_importance.sort_values(\n",
        "    by=['Absolute Coefficient'], ascending=False\n",
        ")\n",
        "\n",
        "# Display the feature importance\n",
        "display(feature_importance)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "fvLe41nZRyZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Model evaluation\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the performance of the optimized linear regression model on the testing dataset.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5cQXajjR1-q"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Evaluate the performance of the optimized linear regression model on the testing dataset by making predictions using the best model found during optimization and calculating the MAE, RMSE, and R2 score.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "BuKvTr0_R2yA"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "The `squared=False` argument is not valid for the `mean_squared_error` function in the current version of scikit-learn. To calculate the RMSE, we need to calculate the MSE and then take the square root manually.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "OL7YzP6fR6dN"
      }
    },
    {
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Separate the target variable and predictor variables for the test set\n",
        "X_test = df_test.drop(['Precio normal', 'month', 'day_of_week', 'year', 'quarter'], axis=1)\n",
        "y_test = df_test['Precio normal']\n",
        "\n",
        "# Use the best model to make predictions on the testing dataset\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Calculate the evaluation metrics\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the calculated evaluation metrics\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
        "print(f\"R-squared (R2): {r2}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "-pFsJl5BR6tB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Data visualization\n",
        "\n",
        "### Subtask:\n",
        "Visualize the predicted consumption trends against the actual consumption data.\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "WWVprt4MR957"
      }
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Create a scatter plot to visualize the model's accuracy in predicting the prices, and a line plot to visualize the actual and predicted prices over time. Since the 'Día en que empieza el precio rebajado' is missing for all the data, we can only plot the scatter plot.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "czFniJ7BR-x9"
      }
    },
    {
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Separate the target variable and predictor variables for the test set\n",
        "X_test = df_test.drop(['Precio normal', 'month', 'day_of_week', 'year', 'quarter'], axis=1)\n",
        "y_test = df_test['Precio normal']\n",
        "\n",
        "# Use the best model to make predictions on the testing dataset\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# 1. Scatter plot: Actual vs. Predicted 'Precio normal'\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "plt.xlabel(\"Actual Precio normal\")\n",
        "plt.ylabel(\"Predicted Precio normal\")\n",
        "plt.title(\"Actual vs. Predicted Precio normal\")\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "0I1ycgo8R_Bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Generate a histogram to visualize the distribution of prediction errors.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "LDaonUA3SAji"
      }
    },
    {
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate prediction errors\n",
        "errors = y_test - y_pred\n",
        "\n",
        "# 3. Histogram: Distribution of prediction errors\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(errors, bins=20)\n",
        "plt.xlabel(\"Prediction Error\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Distribution of Prediction Errors\")\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "hIMEOJwqSAzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "**Reasoning**:\n",
        "Create a bar chart to visualize the average sales predicted by the model for different product categories.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "XY3_SIwUSCd8"
      }
    },
    {
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'product_category' and 'Precio normal' are available in df_test\n",
        "if 'product_category' in df_test.columns and 'Precio normal' in df_test.columns:\n",
        "    # Group by product category and calculate the mean of 'Precio normal' for actual and predicted values\n",
        "    mean_actual_sales_by_category = df_test.groupby('product_category')['Precio normal'].mean()\n",
        "    # Make predictions for the test set\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    df_test['predicted_sales'] = y_pred\n",
        "    mean_predicted_sales_by_category = df_test.groupby('product_category')['predicted_sales'].mean()\n",
        "\n",
        "    # Create a bar chart to visualize average sales by category\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    width = 0.35\n",
        "    x = range(len(mean_actual_sales_by_category))\n",
        "    plt.bar(\n",
        "        [i - width / 2 for i in x],\n",
        "        mean_actual_sales_by_category,\n",
        "        width,\n",
        "        label='Actual Sales',\n",
        "    )\n",
        "    plt.bar(\n",
        "        [i + width / 2 for i in x],\n",
        "        mean_predicted_sales_by_category,\n",
        "        width,\n",
        "        label='Predicted Sales',\n",
        "    )\n",
        "    plt.xticks(x, mean_actual_sales_by_category.index, rotation=45, ha='right')\n",
        "    plt.xlabel('Product Category')\n",
        "    plt.ylabel('Average Sales')\n",
        "    plt.title('Average Predicted Sales vs. Actual Sales by Product Category')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "WJzyBjQUSCtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Extraemos las categorías de productos conocidas\n",
        "categorias_entrenadas = df_train['product_category'].unique().tolist()\n",
        "\n",
        "def predict_sales():\n",
        "    # Entrada del usuario\n",
        "    nombre_producto = input(\"Ingrese el nombre del producto: \").strip().lower()\n",
        "\n",
        "    # Verificar si el producto pertenece a una categoría conocida\n",
        "    if nombre_producto not in categorias_entrenadas:\n",
        "        print(f\"Error: El producto '{nombre_producto}' no pertenece a una categoría en la que el modelo fue entrenado.\")\n",
        "        print(\"Por favor, ingrese un producto de las siguientes categorías:\")\n",
        "        print(\", \".join(categorias_entrenadas))\n",
        "        return  # Terminar la ejecución si la categoría no es válida\n",
        "\n",
        "    precio_normal = float(input(\"Ingrese el precio normal del producto: \"))\n",
        "    precio_rebajado = float(input(\"Ingrese el precio rebajado del producto (si no tiene rebaja, ingrese el mismo precio que el normal): \"))\n",
        "    inventario = int(input(\"Ingrese el inventario disponible: \"))\n",
        "    fecha_inicio_descuento = input(\"Ingrese la fecha de inicio del descuento (formato YYYY-MM-DD): \").strip()\n",
        "\n",
        "    # Determinar si el producto está en rebaja\n",
        "    en_rebaja = precio_rebajado < precio_normal\n",
        "    precio_final = precio_rebajado if en_rebaja else precio_normal  # Usar el precio correcto\n",
        "\n",
        "    # Procesar la fecha de inicio del descuento\n",
        "    fecha_inicio_descuento = datetime.strptime(fecha_inicio_descuento, \"%Y-%m-%d\")\n",
        "\n",
        "    # Crear un DataFrame con los datos proporcionados\n",
        "    input_data = pd.DataFrame({\n",
        "        'product_category': [nombre_producto],\n",
        "        'Precio normal': [precio_normal],\n",
        "        'Precio rebajado': [precio_rebajado],\n",
        "        'Precio final': [precio_final],\n",
        "        'Inventario': [inventario],\n",
        "        'Día en que empieza el precio rebajado': [fecha_inicio_descuento],\n",
        "        'on_sale': [en_rebaja]\n",
        "    })\n",
        "\n",
        "    # Extraer características de fecha\n",
        "    input_data['month'] = input_data['Día en que empieza el precio rebajado'].dt.month\n",
        "    input_data['day_of_week'] = input_data['Día en que empieza el precio rebajado'].dt.dayofweek\n",
        "    input_data['year'] = input_data['Día en que empieza el precio rebajado'].dt.year\n",
        "    input_data['quarter'] = input_data['Día en que empieza el precio rebajado'].dt.quarter\n",
        "\n",
        "    # Hacer la predicción con el modelo entrenado\n",
        "    predicted_sales = pipeline.predict(input_data)\n",
        "    ventas_totales = predicted_sales[0] * inventario\n",
        "    umbral_ventas = 1000  # Ajustar este umbral según los datos históricos\n",
        "\n",
        "    # Determinar si el producto se venderá bien\n",
        "    venta_estado = \"Este producto no se venderá bien.\" if ventas_totales > umbral_ventas else \"¡Sí, este producto probablemente se venderá!\"\n",
        "\n",
        "    # Mostrar resultados\n",
        "    print(f\"La predicción para el producto '{nombre_producto}' es que las ventas estimadas serán: {predicted_sales[0]:.2f}\")\n",
        "    print(f\"Las ventas totales estimadas para este producto son: {ventas_totales:.2f}\")\n",
        "    print(venta_estado)\n",
        "\n",
        "# Llamar a la función para realizar la predicción\n",
        "predict_sales()\n",
        "\n"
      ],
      "metadata": {
        "id": "_Elvb4z4eyAL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}